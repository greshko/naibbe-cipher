{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db19b02-02b4-4530-9faa-73a271207721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 10 runs; 8 successful.\n",
      "Sweep log saved to: voynichesque_sweep_log.csv\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2025, Michael A. Greshko\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software, datasets, and associated documentation files (the \"Software\n",
    "# and Datasets\"), to deal in the Software and Datasets without restriction,\n",
    "# including without limitation the rights to use, copy, modify, merge, publish,\n",
    "# distribute, sublicense, and/or sell copies of the Software and Datasets, and to\n",
    "# permit persons to whom the Software is furnished to do so, subject to the\n",
    "# following conditions:\n",
    "# \n",
    "# - The above copyright notice and this permission notice shall be included\n",
    "#   in all copies or substantial portions of the Software and Datasets.\n",
    "# - Any publications making use of the Software and Datasets, or any substantial\n",
    "#   portions thereof, shall cite the Software and Datasets's original publication:\n",
    "# \n",
    "# > Greshko, Michael A. (2025). The Naibbe cipher: a substitution cipher that encrypts\n",
    "# Latin and Italian as Voynich Manuscript-like ciphertext.\n",
    "# Cryptologia. https://doi.org/10.1080/01611194.2025.2566408\n",
    "#   \n",
    "# THE SOFTWARE AND DATASETS ARE PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
    "# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
    "# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO\n",
    "# EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n",
    "# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    "# THE SOFTWARE AND DATASETS.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "NOTE: The code in 'alphabet_code' is URL-safe base64 of compact JSON:\n",
    "  {\"v\":1,\"A1\":[...26 glyphs...],\"A2\":[...],\"A3\":[...]}\n",
    "\n",
    "To decode the alphabet code, in the event you want to check a given Voynichesque variant's cipher alphabets:\n",
    "    import base64, json\n",
    "    s = base64.urlsafe_b64decode(code + '===').decode()\n",
    "    payload = json.loads(s)  # payload[\"A1\"], payload[\"A2\"], payload[\"A3\"] in A..Z order\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "import base64\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import defaultdict, Counter, namedtuple\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "# ========= PATHS =========\n",
    "CSV_PATH = \"references/voynichesque_alphabet_options.csv\"\n",
    "LOG_CSV_PATH = \"voynichesque_sweep_log.csv\"\n",
    "\n",
    "# ========= GLYPH CLUSTERS (metrics parsing only) =========\n",
    "GLYPH_CLUSTERS = [\"cfh\", \"ckh\", \"cph\", \"cth\", \"sh\", \"ch\"]  # longest-first\n",
    "\n",
    "# ========= DATA MODELS =========\n",
    "Option = namedtuple(\"Option\", [\"code\", \"alphabet\", \"glyph\", \"length\"])\n",
    "\n",
    "@dataclass\n",
    "class VoynParams:\n",
    "    glyph_len_ratio: List[int]  # [r1, r2, r3, r4], sum=20\n",
    "    u_prob: float\n",
    "    t_prob: float\n",
    "    b_prob: float\n",
    "    prob_bigram_B: float\n",
    "    prob_unigram_A2: float\n",
    "    prob_null_y: float\n",
    "\n",
    "\n",
    "# ========= CSV LOADING =========\n",
    "def _normalize_alphabet_field(val) -> int:\n",
    "    s = str(val).strip().lower()\n",
    "    for k in (\"alphabet\", \"alpha\", \"a\"):\n",
    "        s = s.replace(k, \"\").strip()\n",
    "    digits = \"\".join(ch for ch in s if ch.isdigit())\n",
    "    if digits:\n",
    "        return int(digits)\n",
    "    parts = s.split()\n",
    "    for p in parts[::-1]:\n",
    "        if p.isdigit():\n",
    "            return int(p)\n",
    "    raise ValueError(f\"Cannot parse alphabet field: {val!r}\")\n",
    "\n",
    "def load_options(csv_path: str) -> List[Option]:\n",
    "    options: List[Option] = []\n",
    "    with open(csv_path, newline='', encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if not reader.fieldnames:\n",
    "            raise ValueError(\"CSV has no header row.\")\n",
    "        headers = {h.lower(): h for h in reader.fieldnames}\n",
    "        alph_col   = headers.get(\"alphabet\")\n",
    "        glyph_col  = headers.get(\"glyph\") or headers.get(\"glyphs\") or headers.get(\"string\")\n",
    "        length_col = headers.get(\"length\") or headers.get(\"len\")\n",
    "        code_col   = headers.get(\"code\") or list(reader.fieldnames)[0]\n",
    "        if not (alph_col and glyph_col and length_col):\n",
    "            raise ValueError(\"CSV missing required headers: 'alphabet', 'glyph', 'length'.\")\n",
    "        for row in reader:\n",
    "            code   = (row.get(code_col, \"\") or \"\").strip()\n",
    "            alph   = _normalize_alphabet_field(row[alph_col])\n",
    "            glyph  = row[glyph_col].strip()\n",
    "            length = int(row[length_col])\n",
    "            options.append(Option(code, alph, glyph, length))\n",
    "    return options\n",
    "\n",
    "\n",
    "# ========= LENGTH QUOTAS FROM RATIOS =========\n",
    "def quotas_from_ratio(ratio: List[int], total: int = 26) -> List[int]:\n",
    "    if sum(ratio) <= 0:\n",
    "        raise ValueError(\"glyph_len_ratio must have positive sum.\")\n",
    "    base = [r / sum(ratio) * total for r in ratio]\n",
    "    floors = [math.floor(x) for x in base]\n",
    "    remainder = total - sum(floors)\n",
    "    fracs = sorted([(base[i] - floors[i], i) for i in range(len(base))],\n",
    "                   key=lambda t: (-t[0], t[1]))\n",
    "    for _, i in fracs[:remainder]:\n",
    "        floors[i] += 1\n",
    "    return floors  # sums exactly to total\n",
    "\n",
    "\n",
    "# ========= ALPHABET BUILDING =========\n",
    "def build_cipher_alphabet(options: List[Option], alphabet_id: int, length_ratio: List[int]) -> Dict[str, str]:\n",
    "    pool = [opt for opt in options if opt.alphabet == alphabet_id]\n",
    "    bins: Dict[int, List[str]] = defaultdict(list)\n",
    "    for opt in pool:\n",
    "        if 1 <= opt.length <= 4:\n",
    "            bins[opt.length].append(opt.glyph)\n",
    "\n",
    "    quotas = quotas_from_ratio(length_ratio, total=26)   # lengths [1,2,3,4]\n",
    "    for L, needed in enumerate(quotas, start=1):\n",
    "        have = len(bins[L])\n",
    "        if have < needed:\n",
    "            raise ValueError(\n",
    "                f\"Not enough options for Alphabet {alphabet_id}, length {L}: need {needed}, have {have}.\"\n",
    "            )\n",
    "\n",
    "    selected: List[str] = []\n",
    "    for L, needed in enumerate(quotas, start=1):\n",
    "        ranked = sorted(bins[L], key=lambda _: random.random())\n",
    "        selected.extend(ranked[:needed])\n",
    "\n",
    "    ranks = sorted(((random.random(), g) for g in selected))\n",
    "    ranked_glyphs = [g for _, g in ranks]\n",
    "    letters = list(string.ascii_uppercase)  # A..Z\n",
    "    return dict(zip(letters, ranked_glyphs))\n",
    "\n",
    "\n",
    "# ========= VOYNICHESQUE PIPELINE =========\n",
    "alphabet1: Dict[str, str] = {}\n",
    "alphabet2: Dict[str, str] = {}\n",
    "alphabet3_base: Dict[str, str] = {}\n",
    "alphabet3: Dict[str, str] = {}\n",
    "alphabet3_unigram: Dict[str, str] = {}\n",
    "\n",
    "U_PROB = 0.3\n",
    "T_PROB = 0.3\n",
    "B_PROB = 0.4\n",
    "PROB_BIGRAM_B = 0.5\n",
    "PROB_UNIGRAM_A2 = 0.5\n",
    "PROB_NULL_Y = 0.1\n",
    "\n",
    "def respace_text(text: str, u: float, b: float, t: float) -> List[str]:\n",
    "    filtered = ''.join(ch for ch in text.lower() if 'a' <= ch <= 'z')\n",
    "    idx = 0\n",
    "    tokens: List[str] = []\n",
    "    n_total = len(filtered)\n",
    "    while idx < n_total:\n",
    "        r = random.random()\n",
    "        if r < u:\n",
    "            n = 1\n",
    "        elif r < u + b:\n",
    "            n = 2\n",
    "        else:\n",
    "            n = 3\n",
    "        tokens.append(filtered[idx: idx + n])\n",
    "        idx += n\n",
    "    return [tok for tok in tokens if tok]\n",
    "\n",
    "def encrypt_token(token: str) -> str:\n",
    "    L = len(token)\n",
    "    if L == 1:\n",
    "        use_a2 = (random.random() <= PROB_UNIGRAM_A2)\n",
    "        table = alphabet2 if use_a2 else alphabet3_unigram\n",
    "        glyph = table[token.upper()]\n",
    "    elif L == 2:\n",
    "        use_B = (random.random() <= PROB_BIGRAM_B)  # B: A1+A3  |  A: A2+A3\n",
    "        first = (alphabet1 if use_B else alphabet2)[token[0].upper()]\n",
    "        second = alphabet3[token[1].upper()]\n",
    "        glyph = first + second\n",
    "    elif L == 3:\n",
    "        glyph = (\n",
    "            alphabet1[token[0].upper()] +\n",
    "            alphabet2[token[1].upper()] +\n",
    "            alphabet3[token[2].upper()]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Voynichesque supports unigram, bigram, trigram tokens only.\")\n",
    "    if random.random() < PROB_NULL_Y:\n",
    "        glyph += 'y'\n",
    "    return glyph\n",
    "\n",
    "def voynichesque_encrypt(text: str) -> str:\n",
    "    tokens = respace_text(text, U_PROB, B_PROB, T_PROB)\n",
    "    return ' '.join(encrypt_token(tok) for tok in tokens)\n",
    "\n",
    "\n",
    "# ========= PARAMETER SWEEP =========\n",
    "def sample_glyph_len_ratio() -> List[int]:\n",
    "    while True:\n",
    "        r4 = random.randint(0, 2)\n",
    "        rem = 20 - r4\n",
    "        r1_max = min(10, rem)\n",
    "        r1 = random.randint(0, r1_max)\n",
    "        rem2 = rem - r1\n",
    "        r2_max = min(13, rem2)\n",
    "        r2 = random.randint(0, r2_max)\n",
    "        r3 = rem2 - r2\n",
    "        if 0 <= r3 <= 13:\n",
    "            return [r1, r2, r3, r4]\n",
    "\n",
    "def sample_respace_probs() -> Tuple[float, float, float]:\n",
    "    u = random.random() * (0.98)\n",
    "    t = random.random() * (1.0 - u)\n",
    "    b = 1.0 - (u + t)\n",
    "    return u, t, b\n",
    "\n",
    "def sample_other_probs() -> Tuple[float, float, float]:\n",
    "    return random.random(), random.random(), random.random()\n",
    "\n",
    "@dataclass\n",
    "class _ParamsWrap:\n",
    "    params: VoynParams\n",
    "\n",
    "def sample_params() -> VoynParams:\n",
    "    r = sample_glyph_len_ratio()\n",
    "    u, t, b = sample_respace_probs()\n",
    "    pB, pA2, pY = sample_other_probs()\n",
    "    return VoynParams(\n",
    "        glyph_len_ratio=r,\n",
    "        u_prob=u, t_prob=t, b_prob=b,\n",
    "        prob_bigram_B=pB,\n",
    "        prob_unigram_A2=pA2,\n",
    "        prob_null_y=pY\n",
    "    )\n",
    "\n",
    "def run_voynichesque_once(\n",
    "    plaintext: str,\n",
    "    options: List[Option],\n",
    "    rng_seed: Optional[int] = None,\n",
    "    params: Optional[VoynParams] = None\n",
    ") -> Tuple[str, Tuple[Dict[str, str], Dict[str, str], Dict[str, str]], Dict[str, Any]]:\n",
    "    if rng_seed is not None:\n",
    "        random.seed(rng_seed)\n",
    "    if params is None:\n",
    "        params = sample_params()\n",
    "\n",
    "    global U_PROB, T_PROB, B_PROB, PROB_BIGRAM_B, PROB_UNIGRAM_A2, PROB_NULL_Y\n",
    "    U_PROB = params.u_prob\n",
    "    T_PROB = params.t_prob\n",
    "    B_PROB = params.b_prob\n",
    "    PROB_BIGRAM_B   = params.prob_bigram_B\n",
    "    PROB_UNIGRAM_A2 = params.prob_unigram_A2\n",
    "    PROB_NULL_Y     = params.prob_null_y\n",
    "\n",
    "    ratio = params.glyph_len_ratio\n",
    "    a1 = build_cipher_alphabet(options, alphabet_id=1, length_ratio=ratio)\n",
    "    a2 = build_cipher_alphabet(options, alphabet_id=2, length_ratio=ratio)\n",
    "    a3_base = build_cipher_alphabet(options, alphabet_id=3, length_ratio=ratio)\n",
    "\n",
    "    global alphabet1, alphabet2, alphabet3, alphabet3_base, alphabet3_unigram\n",
    "    alphabet1 = a1\n",
    "    alphabet2 = a2\n",
    "    alphabet3_base = a3_base\n",
    "    alphabet3 = alphabet3_base\n",
    "    alphabet3_unigram = {k: 'd' + v for k, v in alphabet3_base.items()}\n",
    "\n",
    "    ciphertext = voynichesque_encrypt(plaintext)\n",
    "    return ciphertext, (a1, a2, a3_base), asdict(params)\n",
    "\n",
    "def sweep_voynichesque(\n",
    "    plaintext: str,\n",
    "    options: List[Option],\n",
    "    n_runs: int = 20,\n",
    "    rng_seed: Optional[int] = None,\n",
    "    keep_alphabets: bool = False\n",
    ") -> List[Dict[str, Any]]:\n",
    "    if rng_seed is not None:\n",
    "        random.seed(rng_seed)\n",
    "    results = []\n",
    "    for _ in range(n_runs):\n",
    "        try:\n",
    "            params = sample_params()\n",
    "            ct, alphs, p = run_voynichesque_once(\n",
    "                plaintext=plaintext,\n",
    "                options=options,\n",
    "                rng_seed=None,\n",
    "                params=params\n",
    "            )\n",
    "            results.append({\n",
    "                \"params\": p,\n",
    "                \"ciphertext\": ct,\n",
    "                \"alphabets\": alphs if keep_alphabets else None,\n",
    "                \"error\": None\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"params\": asdict(params) if 'params' in locals() else None,\n",
    "                \"ciphertext\": None,\n",
    "                \"alphabets\": None,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    return results\n",
    "\n",
    "\n",
    "# ========= GLYPH-AWARE METRICS =========\n",
    "def parse_into_glyphs(text: str) -> List[str]:\n",
    "    glyphs: List[str] = []\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "    while i < n:\n",
    "        for cluster in GLYPH_CLUSTERS:\n",
    "            if text.startswith(cluster, i):\n",
    "                glyphs.append(cluster)\n",
    "                i += len(cluster)\n",
    "                break\n",
    "        else:\n",
    "            glyphs.append(text[i])\n",
    "            i += 1\n",
    "    return glyphs\n",
    "\n",
    "def token_lengths(tokens: List[str]) -> List[int]:\n",
    "    return [len(parse_into_glyphs(tok)) for tok in tokens]\n",
    "\n",
    "def pct_len_hist(lengths: List[int], max_len: int = 12) -> Dict[str, float]:\n",
    "    if not lengths:\n",
    "        return {f\"len{L}_pct\": 0.0 for L in range(1, max_len+1)}\n",
    "    capped = [L if L <= max_len else max_len for L in lengths]\n",
    "    counts = Counter(capped)\n",
    "    total = len(capped)\n",
    "    return {f\"len{L}_pct\": counts.get(L, 0) / total for L in range(1, max_len+1)}\n",
    "\n",
    "def shannon_entropy_glyph(ciphertext: str) -> float:\n",
    "    all_glyphs: List[str] = []\n",
    "    for tok in ciphertext.split():\n",
    "        all_glyphs.extend(parse_into_glyphs(tok))\n",
    "    if not all_glyphs:\n",
    "        return 0.0\n",
    "    counts = Counter(all_glyphs)\n",
    "    n = len(all_glyphs)\n",
    "    return -sum((c/n) * math.log2(c/n) for c in counts.values())\n",
    "\n",
    "def conditional_entropy_glyph(ciphertext: str) -> float:\n",
    "    all_glyphs: List[str] = []\n",
    "    for tok in ciphertext.split():\n",
    "        all_glyphs.extend(parse_into_glyphs(tok))\n",
    "    if len(all_glyphs) < 2:\n",
    "        return 0.0\n",
    "    prev_counts = Counter()\n",
    "    trans_counts = Counter()\n",
    "    for i in range(len(all_glyphs) - 1):\n",
    "        a, b = all_glyphs[i], all_glyphs[i+1]\n",
    "        prev_counts[a] += 1\n",
    "        trans_counts[(a, b)] += 1\n",
    "    n_prev = sum(prev_counts.values())\n",
    "    H = 0.0\n",
    "    for a, na in prev_counts.items():\n",
    "        pa = na / n_prev\n",
    "        nexts = {b: c for (aa, b), c in trans_counts.items() if aa == a}\n",
    "        Ha = 0.0\n",
    "        for b, cab in nexts.items():\n",
    "            p_b_given_a = cab / na\n",
    "            Ha += -p_b_given_a * math.log2(p_b_given_a)\n",
    "        H += pa * Ha\n",
    "    return H\n",
    "\n",
    "def summarize_ciphertext(ciphertext: str) -> Dict[str, Any]:\n",
    "    tokens = [t for t in ciphertext.split() if t]\n",
    "    types = sorted(set(tokens))\n",
    "    tlens = token_lengths(tokens)\n",
    "    typelens = token_lengths(types)\n",
    "\n",
    "    n_tokens = len(tokens)\n",
    "    n_types = len(types)\n",
    "    ttr = (n_types / n_tokens) if n_tokens else 0.0\n",
    "    mean_len = (sum(tlens) / n_tokens) if n_tokens else 0.0\n",
    "    if n_tokens:\n",
    "        sl = sorted(tlens)\n",
    "        median_len = sl[n_tokens // 2] if n_tokens % 2 == 1 else (sl[n_tokens // 2 - 1] + sl[n_tokens // 2]) / 2\n",
    "    else:\n",
    "        median_len = 0.0\n",
    "\n",
    "    H = shannon_entropy_glyph(ciphertext)\n",
    "    Hc = conditional_entropy_glyph(ciphertext)\n",
    "\n",
    "    token_hist = pct_len_hist(tlens, max_len=12)\n",
    "    type_hist  = pct_len_hist(typelens, max_len=12)\n",
    "    type_hist  = {f\"type_len{L}_pct\": type_hist[f\"len{L}_pct\"] for L in range(1, 13)}\n",
    "\n",
    "    return {\n",
    "        \"n_tokens\": n_tokens,\n",
    "        \"n_types\": n_types,\n",
    "        \"ttr\": ttr,\n",
    "        \"mean_token_len\": mean_len,\n",
    "        \"median_token_len\": median_len,\n",
    "        \"char_entropy\": H,\n",
    "        \"cond_char_entropy\": Hc,\n",
    "        **token_hist,\n",
    "        **type_hist\n",
    "    }\n",
    "\n",
    "\n",
    "# ========= ALPHABET CODE (compact, recoverable) =========\n",
    "def _alph_to_list(alpha: Dict[str, str]) -> List[str]:\n",
    "    # A..Z order\n",
    "    return [alpha[ch] for ch in string.ascii_uppercase]\n",
    "\n",
    "def build_alphabet_code(a1: Dict[str, str], a2: Dict[str, str], a3: Dict[str, str]) -> str:\n",
    "    payload = {\"v\": 1, \"A1\": _alph_to_list(a1), \"A2\": _alph_to_list(a2), \"A3\": _alph_to_list(a3)}\n",
    "    s = json.dumps(payload, separators=(',', ':'))\n",
    "    code = base64.urlsafe_b64encode(s.encode(\"utf-8\")).decode(\"utf-8\").rstrip('=')\n",
    "    return code\n",
    "\n",
    "\n",
    "# ========= CSV LOGGING =========\n",
    "def init_csv_logger(path: str) -> List[str]:\n",
    "    fieldnames = [\n",
    "        # params\n",
    "        \"r1\",\"r2\",\"r3\",\"r4\",\n",
    "        \"u_prob\",\"b_prob\",\"t_prob\",\n",
    "        \"prob_bigram_B\",\"prob_unigram_A2\",\"prob_null_y\",\n",
    "        # encoded alphabets\n",
    "        \"alphabet_code\",\n",
    "        # summary stats\n",
    "        \"n_tokens\",\"n_types\",\"ttr\",\"mean_token_len\",\"median_token_len\",\n",
    "        \"char_entropy\",\"cond_char_entropy\",\n",
    "    ] + [f\"len{L}_pct\" for L in range(1,13)] + [f\"type_len{L}_pct\" for L in range(1,13)]\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "    return fieldnames\n",
    "\n",
    "def append_run_to_csv(path: str, params_dict: Dict[str, Any], summary_dict: Dict[str, Any], alphabet_code: str) -> None:\n",
    "    row = {\n",
    "        # params\n",
    "        \"r1\": params_dict[\"glyph_len_ratio\"][0],\n",
    "        \"r2\": params_dict[\"glyph_len_ratio\"][1],\n",
    "        \"r3\": params_dict[\"glyph_len_ratio\"][2],\n",
    "        \"r4\": params_dict[\"glyph_len_ratio\"][3],\n",
    "        \"u_prob\": params_dict[\"u_prob\"],\n",
    "        \"b_prob\": params_dict[\"b_prob\"],\n",
    "        \"t_prob\": params_dict[\"t_prob\"],\n",
    "        \"prob_bigram_B\": params_dict[\"prob_bigram_B\"],\n",
    "        \"prob_unigram_A2\": params_dict[\"prob_unigram_A2\"],\n",
    "        \"prob_null_y\": params_dict[\"prob_null_y\"],\n",
    "        # alphabets\n",
    "        \"alphabet_code\": alphabet_code,\n",
    "        # metrics\n",
    "        **summary_dict\n",
    "    }\n",
    "    fieldnames = [\n",
    "        \"r1\",\"r2\",\"r3\",\"r4\",\n",
    "        \"u_prob\",\"b_prob\",\"t_prob\",\n",
    "        \"prob_bigram_B\",\"prob_unigram_A2\",\"prob_null_y\",\n",
    "        \"alphabet_code\",\n",
    "        \"n_tokens\",\"n_types\",\"ttr\",\"mean_token_len\",\"median_token_len\",\n",
    "        \"char_entropy\",\"cond_char_entropy\",\n",
    "    ] + [f\"len{L}_pct\" for L in range(1,13)] + [f\"type_len{L}_pct\" for L in range(1,13)]\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "# ========= MAIN =========\n",
    "if __name__ == \"__main__\":\n",
    "    SWEEP_SEED = 123\n",
    "    random.seed(SWEEP_SEED)\n",
    "\n",
    "    all_options = load_options(CSV_PATH)\n",
    "    init_csv_logger(LOG_CSV_PATH)\n",
    "\n",
    "        # === Load reference plaintext from TXT file ===\n",
    "    TXT_FILE_PATH = \"input/examples/nathist_book16.txt\"\n",
    "    \n",
    "    with open(TXT_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        plaintext = f.read()\n",
    "    \n",
    "    # Normalize whitespace:\n",
    "    plaintext = \" \".join(plaintext.split())\n",
    "    \n",
    "    # Then run the sweep using that plaintext\n",
    "    runs = sweep_voynichesque(\n",
    "        plaintext=plaintext,\n",
    "        options=all_options,\n",
    "        n_runs=10, # Experimentally, this script is ~75.5% efficient. That is, set n_runs=1000, you'll get ~755 successful runs.\n",
    "        rng_seed=SWEEP_SEED,\n",
    "        keep_alphabets=True # we need alphabets to build the code\n",
    "    )\n",
    "\n",
    "    success = 0\n",
    "    for r in runs:\n",
    "        if r[\"error\"]:\n",
    "            continue\n",
    "        params = r[\"params\"]\n",
    "        ct = r[\"ciphertext\"]\n",
    "        (A1, A2, A3) = r[\"alphabets\"]\n",
    "        summary = summarize_ciphertext(ct)\n",
    "        code = build_alphabet_code(A1, A2, A3)\n",
    "        append_run_to_csv(LOG_CSV_PATH, params, summary, code)\n",
    "        success += 1\n",
    "\n",
    "    print(f\"Completed {len(runs)} runs; {success} successful.\")\n",
    "    print(f\"Sweep log saved to: {LOG_CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e92698-d4c0-4e23-8955-4e48a2f8d786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
